{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71332533",
   "metadata": {},
   "source": [
    "We are all very familiar with Logistic Regression and its optimization solution to minimize its cost function. But in reality, we may need some constraints. The following are two special constraints, please give: a) optimization solution; b) code of the solution process; c) sample code (you can use the sklearn data set or generate your own code)\n",
    "1) Non-negative constraint: All coefficients of LR are required to be non-negative;\n",
    "2) Order preserving constraint: The coefficient of LR is required to satisfy a1 >= a2 >= a3 ……\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b1c19",
   "metadata": {},
   "source": [
    "In the following, I edit a from-scratch Logistic Regression algorithm in the function coefficients_sgd to (1) ensure positive coefficients using absolute functions and (2) ensure positive and weights descending in strength from the first X term using absolute and np.sort functions \n",
    "\n",
    "For (1) and (2), I also vary the learning rate for each iteration to try to 'jiggle' gradient descent out of getting stuck in local maximas.\n",
    "\n",
    "I provide my answers to the questions at the bottom. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3d419",
   "metadata": {},
   "source": [
    "Pima Indians Diabetes Dataset\n",
    "The Pima Indians dataset involves predicting the onset of diabetes within 5 years in Pima Indians given basic medical details.\n",
    "\n",
    "Dataset File.\n",
    "Dataset Details.\n",
    "It is a binary classification problem, where the prediction is either 0 (no diabetes) or 1 (diabetes).\n",
    "\n",
    "It contains 768 rows and 9 columns. All of the values in the file are numeric, specifically floating point values. Below is a small sample of the first few rows of the problem.\n",
    "\n",
    "Predicting the majority class, the baseline performance on this problem is 65.098% classification accuracy.\n",
    "\n",
    "We see later that our constriained algorithm produces accuracies of about 34%, about 48% drop in performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6142fc",
   "metadata": {},
   "source": [
    "# Positive Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d531f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression on Diabetes Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import exp\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    " \n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    " \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    " \n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return 1.0 / (1.0 + exp(-yhat))\n",
    " \n",
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = row[-1] - yhat\n",
    "            # I use absolute functions here to ensure positive coefficients\n",
    "            coef[0] = abs(coef[0]) + l_rate * error * yhat * (1.0 - yhat)\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "                coef = [abs(i) for i in coef]\n",
    "                \n",
    "    return coef\n",
    " \n",
    "# Linear Regression Algorithm With Stochastic Gradient Descent\n",
    "def logistic_regression(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    coef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "    print(\"coefficients for each kfold step are positive - \" + str(coef))\n",
    "    for row in test:\n",
    "        yhat = predict(row, coef)\n",
    "        yhat = round(yhat)\n",
    "        predictions.append(yhat)\n",
    "        \n",
    "    return(predictions)\n",
    " \n",
    "def run_test1():\n",
    "    # Test the logistic regression algorithm on the diabetes dataset\n",
    "    seed(1)\n",
    "    # load and prepare data\n",
    "    filename = 'pima-indians-diabetes.csv'\n",
    "    dataset = load_csv(filename)\n",
    "    for i in range(len(dataset[0])):\n",
    "        str_column_to_float(dataset, i)\n",
    "    # normalize\n",
    "    minmax = dataset_minmax(dataset)\n",
    "    normalize_dataset(dataset, minmax)\n",
    "    # evaluate algorithm\n",
    "    n_folds = 5\n",
    "    l_rate = 0.1\n",
    "    n_epoch = 100\n",
    "    scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch)\n",
    "    print('Scores: %s' % scores)\n",
    "    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6cfb978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients for each kfold step are positive - [0.0003551784706798695, 0.02881803671570453, 0.0006707048505025731, 0.002852099842011303, 0.005523343823740465, 0.003071089742031467, 0.006896414169791931, 0.008424637707320935, 0.01470274052357582]\n",
      "coefficients for each kfold step are positive - [0.00042026942840225033, 0.028677363048408233, 0.0007226294738801355, 0.0027682902867737077, 0.00553033782415361, 0.0030750891619692323, 0.006934607966704234, 0.008418516521555114, 0.014683627261389784]\n",
      "coefficients for each kfold step are positive - [0.0004267357993742553, 0.02865753474554763, 0.0006945631820236709, 0.0028425954602073486, 0.005547371948074472, 0.0030456977933912346, 0.006876130000905141, 0.008423095398940485, 0.01470706144164734]\n",
      "coefficients for each kfold step are positive - [0.00035976343949581206, 0.028693088178692587, 0.0006066505086812632, 0.002755240624559864, 0.0055227793057408235, 0.0029247510521121156, 0.006863015458226159, 0.00839366754465008, 0.014593167770915856]\n",
      "coefficients for each kfold step are positive - [0.005984427270230366, 0.00930091260664968, 0.006765150352193369, 0.0013031455553905898, 0.008075352002645095, 0.0068813145135232096, 0.005941811109988722, 0.0015183466367151583, 0.014804157047293385]\n",
      "Scores: [37.908496732026144, 35.294117647058826, 35.294117647058826, 35.294117647058826, 30.718954248366014]\n",
      "Mean Accuracy: 34.902%\n"
     ]
    }
   ],
   "source": [
    "run_test1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eed18c",
   "metadata": {},
   "source": [
    "# Positive and Descending Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3bf177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression on Diabetes Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import exp\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    " \n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    " \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    " \n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return 1.0 / (1.0 + exp(-yhat))\n",
    " \n",
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = row[-1] - yhat\n",
    "            \n",
    "            # I use absolute functions here and a sort function to ensure that coefficients are positive and descending in order\n",
    "            coef[0] = abs(coef[0]) + l_rate * error * yhat * (1.0 - yhat)\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "                coef = [abs(i) for i in coef]\n",
    "                coef = sorted(coef, reverse = True)\n",
    "                \n",
    "    return coef\n",
    " \n",
    "# Linear Regression Algorithm With Stochastic Gradient Descent\n",
    "def logistic_regression(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    coef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "    print(\"coefficients for each kfold step are positive AND descending in order - \" + str(coef))\n",
    "    for row in test:\n",
    "        yhat = predict(row, coef)\n",
    "        yhat = round(yhat)\n",
    "        predictions.append(yhat)\n",
    "        \n",
    "    return(predictions)\n",
    " \n",
    "def run_test2():\n",
    "    # Test the logistic regression algorithm on the diabetes dataset\n",
    "    seed(1)\n",
    "    # load and prepare data\n",
    "    filename = 'pima-indians-diabetes.csv'\n",
    "    dataset = load_csv(filename)\n",
    "    for i in range(len(dataset[0])):\n",
    "        str_column_to_float(dataset, i)\n",
    "    # normalize\n",
    "    minmax = dataset_minmax(dataset)\n",
    "    normalize_dataset(dataset, minmax)\n",
    "    # evaluate algorithm\n",
    "    n_folds = 5\n",
    "    l_rate = 0.5\n",
    "    n_epoch = 100\n",
    "    scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch)\n",
    "    print('Scores: %s' % scores)\n",
    "    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2175ad37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients for each kfold step are positive AND descending in order - [0.06249187279469407, 0.048169059868665154, 0.04084932612441459, 0.029452733550942738, 0.025555252374678063, 0.020964617954868814, 0.02009444310507721, 0.013866394913012697, 0.004580970878454833]\n",
      "coefficients for each kfold step are positive AND descending in order - [0.06249102076815966, 0.04817517938682098, 0.04084422447548834, 0.029457379861677298, 0.025556363054482032, 0.02096500091825213, 0.020091908689768297, 0.013868753572509055, 0.004580786449692217]\n",
      "coefficients for each kfold step are positive AND descending in order - [0.06249993902786857, 0.048186151179277395, 0.0408462006349529, 0.029459307431665, 0.025568237012807898, 0.02097462870068146, 0.020088100499477922, 0.013858635355591113, 0.004594322232850927]\n",
      "coefficients for each kfold step are positive AND descending in order - [0.06251911625761436, 0.04817865823570523, 0.040872239214982564, 0.029455661876163275, 0.025512772125654518, 0.020984939769950528, 0.02015461942649019, 0.013922585643038575, 0.00456694788802885]\n",
      "coefficients for each kfold step are positive AND descending in order - [0.06533263841402456, 0.05996207471718963, 0.042033273367577395, 0.04016264493562319, 0.024879613612099793, 0.018453268789870747, 0.012847624568435401, 0.012683972340390794, 0.003028036992011183]\n",
      "Scores: [37.908496732026144, 35.294117647058826, 35.294117647058826, 35.294117647058826, 30.718954248366014]\n",
      "Mean Accuracy: 34.902%\n"
     ]
    }
   ],
   "source": [
    "run_test2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a639894",
   "metadata": {},
   "source": [
    "One more question: If there is no constraint, is LR a global optimization algorithm or a local one? why? When constraints 1) or 2) are imposed, global or local? why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f2b82",
   "metadata": {},
   "source": [
    "QN: IS LR A GLOBAL OPTIMIZATION ALGORITHM\n",
    "\n",
    "Global optimization refers to finding the optimal value of a given function among all possible solutions whereas local optimization finds the optimal solution for a specific region of the search space (or global optima for problems with no local optima)\n",
    "\n",
    "The two can intersect, when the local-minimum is also the global minimum. This is known as convexity. \n",
    "\n",
    "Logistic Regression is a local optimization algorithm intended to locate a local optima. It operates on a single candidate solution and involves iteratively making small changes to the candidate solution and evaluating the change to see if it leads to an improvement and is taken as the new candidate solution. \n",
    "\n",
    "However, despite it being a local optimization algorithm, it can locate the global optimum (1) if the local optima is the global optima or (2) the region being searched contains the global optima. \n",
    "\n",
    "In contrast, a global optimization algorithm is intended to locate the global optima. It will traverse the ENTIRE input space to find the optima. It does so by managing population of candidate solutions from which new candidate solutions are iteratively generated. As we see from the above solution, Logistic Regression does not fit this description as only one candidate solution is iterated and evaluated. \n",
    "\n",
    "QN: WHEN CONSTRAINTS ARE IMPOSED, GLOBAL OR LOCAL? \n",
    "\n",
    "When contraints are imposed, LR REMAINS a local optimization algorithm. In FACT, it becomes even MORE local, because we have (in the case of our constraints 1 and 2), drastically reduced the input search space that we can traverse. What has happened is that we have reduced the convexity of the problem, where we are no longer guaranteed that the local minimum is also the global minimum.\n",
    "\n",
    "So LR was never a global optimization algorithm, just a local one that is often able to find the global minimum by consequence. With constraints, we have possibly broken this link, and may have to be satisfied with local minimas that are non-global optimal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
